## Attention

### 등장 배경

#### 1. 기존 모델(RNN, LSTM, GRU)의 한계

- 딥러닝 초기에는 RNN(Recurrent Neural Network) 및 그 변형인 LSTM(Long Short-Term Memory), GRU(Gated Recurrent Unit) 등이 자연어 처리에 널리 사용됨

<br />

- RNN/LSTM의 주요 한계

    1. 긴 문장에서 정보 손실(Vanishing Gradient Problem)

        - RNN은 입력 문장을 앞에서부터 차례대로 처리하는 구조

        - 문장이 길어질수록 초반 단어의 정보가 소실되며 마지막 단어를 예측할 때 영향을 거의 미치지 못함
    
    2. 병렬 연산 불가능(Sequential Processing)

        - RNN은 이전 단계의 출력을 다음 단계의 입력으로 사용하기 때문에, 병렬 연산이 불가능하고 계산 속도가 느림
    
    3. 장거리 의존성(Long-Term Dependency) 문제

        - ```"The cat, which was sleeping on the mat, suddenly woke up."``` -> ```"woke up"```을 예측하려면 ```"cat"```과의 관계를 유지해야 하지만, 중간에 많은 단어가 있어서 기억하기 어려움

<br />

#### 2. Attention 매커니즘의 등장 (2014)

- 이러한 문제를 해결하기 위해 2014년 Bahdanau et al의 논문 "Neural Machine Translation by Jointly Learning to Align and Translate"에서 Attention 매커니즘이 제안됨

- Attention의 핵심 개념

    - 문장을 번역할 때, 전체 입력 문장에서 중요한 단어에 더 집중할 수 있도록 가중치를 부여하는 방식

    - 사람도 변역할 때 모든 단어를 한 번에 보는 것이 아니라, 특정 단어에 집중하면서 이해하는 것과 같은 방식

- Attention의 장점

    1. 중요한 단어에 집중 -> 긴 문장에서도 장거리 의존성 문제 해결

    2. 병렬 연산 가능 -> 특정 단어만 선택적으로 처리하므로 RNN보다 빠른 연산 가능

    3. 모든 단어를 참조하면서 번역 가능 -> 정보 손실 감소

<br />

#### 3. Self-Attention과 Transformer 모델의 등장 (2017)

- Attention은 원래 RNN과 함께 사용되는 방식이었지만, 2017년 Google의 Vaswani et al 논문 "Attention is All You Need"에서 RNN 없이 오직 Self-Attention을 기반으로 하는 "Transformer" 모델을 제안

- Transformer 모델의 핵심

    - 기존 RNN을 제거하고, 완전히 Attention 매커니즘만을 사용하여 자연어를 처리

    - Self-Attention을 도입하여 모든 단어가 서로의 관계를 고려할 수 있도록 함

    - 멀티 헤드 어텐션(Multi-Head Attention)을 통해 다양한 패턴을 학습할 수 있도록 개선

    <br />

    ![attention vs self attention](https://ffighting.net/wp-content/uploads/2023/05/image-24-1536x702.png)

    <br />
- Transformer의 장점

    - RNN보다 훨씬 빠르고 효율적

    - 병렬 연산 가능 -> GPU에서 대량 데이터 처리 가능

    - 장거리 의존성 문제 해결

        <br />

        ![transformer의 큰 그림](https://ffighting.net/wp-content/uploads/2023/05/image-25.png)

        <br />

        <br />

        ![transformer 전체 그림](https://ffighting.net/wp-content/uploads/2023/05/image-26.png)

        <br />

- 이후 Transformer 모델은 BERT, GPT, T5, BART 등 현대 AI 모델의 기반이 됨




<br />

## 번역 모델은 Attention 등장 전과 후가 크게 다르다

- 이전에는 RNN(LSTM, GRU) 기반의 모델이 주류였지만 현재는 거의 모든 번역 시스템이 Attention을 사용하는 Transformer 모델을 기반으로 함

<br />

### 1. Attention 이전의 번역 방식

1) 통계적 기계 번역(Statistical Machine Translation, SMT) (1990s ~ 2010s)

    - 번역을 확률 기반으로 처리

    -  ```"나는 영화를 본다"``` 의 주어진 확률값
        - P(는|나) = 0.9

        - P(영화|나는) = 0.7

        - P(를|나는영화) = 0.94

        - P(본다|나는영화를) = 0.8

        - 전체 문장의 확률 계산
        
            ```P("나는 영화를 본다")``` = ```P(는|나) x P(영화|나는) x P(를|나는영화) x P(본다|나는영화를)``` = 0.9 x 0.7 x 0.94 x 0. 8

    - 대표 모델: IBM Model 1~5, MOSES, Google Translate(초기 버전)

        - 단어 또는 구(phrase) 단위로 번역 확률을 계산하여 번역 수행

        - 언어 모델(Language Model)과 번역 모델(Translation Model)의 결합

        - 대표적인 예로 MOSES (2000년대 초반 오픈 소스 SMT 시스템)가 있음

    - 문제점

        - 문맥을 제대로 반영하지 못함 (ex. "bank" -> 강둑 or 은행)

        - 문장이 길어질수록 번역 품질이 급격히 하락

        - 모델이 복잡하고 데이터 학습 과정이 어려움

<br />

2) RNN 기반의 신경망 기계 번역(NMT) (2014년~2017년)

    - 번역을 뉴럴 네트워크 기반으로 수행

    - 대표 모델: RNN Encoder-Decoder, LSTM, GRU

        - 문장을 입력받은 후 인코더(encoder)가 의미를 압축 (벡터 표현)

        - 디코더(decoder)가 해당 벡터를 기반으로 번역된 문장 생성

        - 학습을 위해 "Teaching Forcing" 기법 사용 (정답을 일부 제공하며 학습)

    - 문제점

        - 장거리 의존성 문제: 문장이 길어질수록 초반 단어의 정보가 사라짐 -> 정보 손실 발생

        - 고정된 컨텍스트 벡터 문제: RNN의 마지막 hidden state만 사용하여 번역 -> 문장 전체를 반영하기 어려움

        - 병렬 연산 불가능: 하나의 단어를 처리한 후 다음 단어를 처리하는 방식 -> 속도가 느림

<br />

### 2. Attention 등장 이후 (2014 ~ 현재)

- 중요한 단어에 가중치를 두어 문맥을 고려한 번역 수행

<br />

1) Attention 기반의 RNN + LSTM 번역 모델 (2014 ~ 2017)

    - 논문 "Neural Machine Translation by Jointly Learning to Alighn and Translate" (Bahdanau et al., 2014)

        - RNN이 마지막 단어의 정보만 기억하는 문제 해결을 위해 각 단어의 중요도를 가중치로 조절

        - 번역할 때 모든 입력 단어를 참조하면서 중요한 부분에 더 집중하는 방식

    
    ![RNN 기반 Attention Seq2seq 구조](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHixMi%2FbtqDlkuE4XV%2FrtvT9hKMMMkxjVDnyggqj0%2Fimg.png)

    

    - 개선점

        - 장거리 의존성 문제 해결 (초반 단어도 끝까지 반영)

        - 더 자연스로운 번역 결과 도출

    - 하지만 RNN을 기반으로 하기 때문에 여전히 속도 느림

<br />

2) Transformer 기반 번역 모델 (2017 ~ 현재)

    - 논문 "Attenetion is All You Need" (Vaswani et al., 2017, Google Brain)

    - RNN 없이 Attention만 사용하여 번역 수행

    - 기존의 순차적 계산 방식(RNN)을 제거하고 병렬 연산 가능하게 함

    - Self-Attention을 활용하여 각 단어가 문장 내 다른 단어들과 관계를 학습

    - 대표 모델: Transformer, BERT, GPT, TS, MarianMT

    - 개선점

        - RNN보다 훨씬 빠름 (병렬 연산 가능)

        - 장거리 문맥을 잘 반영

        - 긴 문장도 자연스럽게 번역 가능

    - 한계

        - 학습 비용이 큼 (많은 연산 필요)

        - 많은 데이터를 필요로 함

### 3. 현재 번역 모델의 트렌드 (2024 기준)

- 현재 번역 모델은 거의 모든 경우 Attention 기반 모델을 사용, 특히 Transformer 기반의 모델이 사실상 표준이 됨

    |모델명|특징|
    |------|---|
    |MarianMT|Facebook이 만든 Transformer 기반 번역 모델|
    |T5 (Text-to-Text Transfer transformer)|번역뿐만 아니라 다양한 NLP 작업 지원|
    |mBART (Multilingual BART)|다국어 번역과 생성 작업에 최적화|
    |NLLB-200 (No Language Left Behine)|Meta가 개발한 200개 언어 지원 번역 모델|
    |GPT-4 (ChatGPT)|번역뿐만 아니라 종합적인 자연어 처리 가능|