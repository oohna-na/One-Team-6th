# Contents
- [Tokenizer](#tokenizer)
    - [Tokenizer 개념](#tokenizer-개념)
    - [Subword Tokenization](#subword-tokenization)
    - [BPE 알고리즘](#bpe-알고리즘)
    - [WordPiece Tokenizer](#wordpiece-tokenizer)

<br>

# Tokenizer

- Tokenizer(토크나이저)는 문장을 작은 단위(토큰)로 나누는 과정을 수행하는 NLP의 필수적인 전처리 과정

<br>

## Tokenizer 개념

- Tokenization(토큰화): 입력된 문장을 단어나 하위 단어(Subword)로 나누는 과정

- Tokenizer를 통해 단어를 쪼개고, 숫자 ID로 변환하여 모델에 입력

- 토큰화의 품질이 NLP 모델 학습(Pre-training) 성능에 큰 영향을 미침

- 즉, 토크나이저는 문장을 기계가 이해할 수 있는 형태로 변환하는 역할

<br>

## Subword Tokenization

- 하위 단어 토큰화

- 왜 Subword Tokenization이 필요할까?

    - 기존 단어 단위 토큰화의 문제점

        - 희귀 단어(OOV, Out-of-Vocabulary) 문제 발생

        - 신조어, 복합어를 모델이 처리하지 못함

    - 해결책

        - 단어를 더 작은 단위(Subword)로 쪼개서 처리

        - ex. ```"Fastcampus"``` -> ```"Fast"```, ```"campus"```, ```"happiness"``` -> ```"happi"```, ```"ness"```

        - 즉, Subword Tokenization을 사용하면 새로운 단어도 쉽게 처리할 수 있음

<br>

## BPE 알고리즘

- BPE(Byte Pair Encoding) 알고리즘은 자주 등장하는 글자 쌍을 병합하여 새로운 단어 집합을 만드는 방식

- Bottom-up 방식 (글자 단위에서 점차적으로 단어를 구성)

- BPE 과정 예시

    1. 초기 단어 리스트: ```"low: 5"```, ```"lower: 2"```, ```"newest: 6"```, ```"widest: 3"```

    2. 각 단어를 글자 단위로 분할:  ```"l o w : 5"```,  ```"l o w e r : 2"```, ```"n e w e s t: 6"```, ```"w i d e s t : 3"```,

    3. 가장 많이 등장하는 글자 쌍을 결합:

        - ```"es"``` -> ```"est"```로 병합

        - ```"lo"``` -> ```"low"```로 병합
    
    4. 최종적으로 새로운 단어 집합이 만들어짐

        - ```"low, newest, widest, lo, ne, wi"```

    즉, BPE는 빈도수가 높은 글자 쌍을 병합하여 점진적으로 단어를 형성하는 방식

<br>

## WordPiece Tokenizer

- WordPiece Tokenizer는 BPE와 비슷하지만, 단순히 빈도수를 기준으로 병합하지 않고 "우도(Likelihood)를 가장 높이는 방식"을 사용

- 즉, 모델이 더 좋은 예측을 할 수 있도록 가장 적절한 토큰을 병합

- WordPiece Tokenization의 특징

    - BERT에서 사용된 토크나이저

    - 빈도수 기반이 아닌 "확률을 고려하여 가장 적절한 단어 쌍을 병합"

    - ex.

        - ```"playing"``` -> ```"play"```가 사전에 존재하면 ```"play"```를 먼저 선택

        - ```"ing"```이 따로 존재하면 ```"##ing"```으로 표현 (##는 앞에 다른 토큰이 존재함을 의미)

        - 최종 토큰화 결과: ```"playing" -> ["play", "##ing"]```

    - 언어 모델의 성능을 최대한 유지하면서 단어를 효과적으로 분할

    - 즉, WordPiece는 BPE보다 더 정교한 방식으로 토큰을 병합하여 모델읠 성능을 최적화함