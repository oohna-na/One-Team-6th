# Contents
- [Sequence-to-sequence](#Sequence-to-sequence)
- [RNN](#RNN)
- [LSTM](#LSTM)
- [GRU](#GRU)



<br>

# Sequence-to-sequence

- Seq2Seq

- 입력 시퀀스를 다른 시퀀스로 변환하는 구조

- 인코더(Encoder)와 디코더(Decoder)로 구성

### 1. 인코더

- 입력 시퀀스를 받아 고정된 길이의 벡터(Context Vector)로 변환

- 문장의 정보를 압축적으로 표현

- 수식 

$$
h_t = f(W_h h_{t-1} + W_x x_t)
$$

- 수식 설명

    - **\( h_t \)** : 현재 시점의 은닉 상태 (Hidden State)
    - **\( h_t-1 \)** : 이전 시간 단계의 은닉 상태
    - **\( W_h, W_x \)** : 학습 가능한 가중치 행렬
    - **\( x_t \)** : 현재 입력값
    - **\( f \)** : 활성화 함수 (ex. tanh)


### 2. 디코더

- 인코더에서 생성한 문맥 벡터(Context Vector)를 기반으로 출력 시퀀스를 생성

- 이전 출력과 문맥 벡터를 사용하여 다음 출력을 예측

### 참고

- 인코더, 디코더에 다양한 모델이 올 수 있음 (RNN, Attention, Self-Attention 등)

- 구조적인 관점에서 자연언어처리 대부분은 Encoder only, Decoder only, Encoder-Decoder 구조라는 이 3가지 방식을 벗어나는 경우 거의 없음

- 자연언어를 이해하는 Encoder 부분과 자연언어 생성을 하는 Decoder 부분으로 나뉨

<br>

# RNN

- Recurrent Neural Networks

- 시퀀스 데이터를 다루기 위한 신경망 구조로, 이전 시간 스텝의 정보를 현재 스텝에 전달하여 시퀀스를 모델링함

<br>

- 수식 

$$
h_t = \sigma(W_h h_{t-1} + W_x x_t)
$$

$$
y_t = g(W_y h_t)
$$

- 수식 설명

    - **\( h_t \)** : 현재 은닉 상태 (메모리 역할)
    - **\( W_h, W_x, W_y \)** : 학습 가능한 가중치
    - **\( y_t \)** : 현재 시점의 출력
    - **\( \sigma, g \)** : 활성화 함수 (보통 tanh, softmax)

<br>

- RNN의 주요 문제점

    - Long-term dependency 문제: 긴 시퀀스를 다룰 때, 과거 정보가 후반부로 잘 전달되지 않음

    - Gradient Vanishing/Exploding: 역전파 시 기울기가 너무 작아지거나 커지는 문제

<br>

# LSTM

- Long Short-Term Memoery

- RNN의 단점을 개선한 구조로, 중요한 정보를 오랫동안 기억할 수 있도록 설계됨

- 수식

    1. Forget Gate: 불필요한 정보를 제거

    $$
    f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
    $$

    2. Input Gate: 새로운 정보를 추가

    $$
    i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
    $$

    - Candidate Cell State:

    $$
    \tilde{C}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)
    $$

    - 최종 Cell State 업데이트:

    $$
    C_t = f_t C_{t-1} + i_t \tilde{C}_t
    $$

    3. Output Gate: 최종적인 출력값 결정

    $$
    o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
    $$

    - Hidden State 업데이트:

    $$
    h_t = o_t \tanh(C_t)
    $$

-  수식 설명
    - **\( f_t \)** : Forget Gate의 활성화 값
    - **\( i_t \)** : Input Gate의 활성화 값
    - **\( C_t \)** : 현재 셀 상태 (Cell State)
    - **\( o_t \)** : Output Gate의 활성화 값
    - **\( h_t \)** : 현재 은닉 상태 (Hidden State)
    - **\( W_f, W_i, W_o, W_c \)** : 학습 가능한 가중치 행렬
    - **\( b_f, b_i, b_o, b_c \)** : 각 게이트의 편향값 (Bias)

<br>

# GRU

- Gated Recurrent Unit

- LSTM의 간소화 버전으로, 계산 효율성이 높은 구조를 가짐

- 수식

    1. Update Gate: 새로운 정보와 이전 정보를 얼마나 섞을지 결정정

    $$
    z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)
    $$

    2. Reset Gate: 이전 정보를 얼마나 반영할지 결정정

    $$
    r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)
    $$

    - Candidate Hidden State:

    $$
    \tilde{h}_t = \tanh(W_h [r_t \cdot h_{t-1}, x_t] + b_h)
    $$

    - 최종 Hidden State 업데이트:

    $$
    h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t
    $$

- 수식 설명

    - **\( z_t \)** : Update Gate의 활성화 값
    - **\( r_t \)** : Reset Gate의 활성화 값
    - **\( \tilde{h}_t \)** : Candidate Hidden State
    - **\( h_t \)** : 현재 은닉 상태 (Hidden State)
    - **\( W_z, W_r, W_h \)** : 학습 가능한 가중치 행렬
    - **\( b_z, b_r, b_h \)** : 각 게이트의 편향값 (Bias)

<br>

# RNN, LSTM, GRU 비교

| 모델  | 게이트 수 | Long-term Dependency | 계산 효율성 |
|-------|----------|--------------|------------|
| **RNN**  | 없음  | 낮음 | 높음 |
| **LSTM** | 3개 (Forget, Input, Output) | 높음 | 낮음 |
| **GRU**  | 2개 (Update, Reset) | 중간 | 중간 |

