# Contents
- [Sequence-to-sequence](#Sequence-to-sequence)
- [RNN](#RNN)
- [LSTM](#LSTM)
- [GRU](#GRU)



<br>

# Sequence-to-sequence

- Seq2Seq

- 입력 시퀀스를 다른 시퀀스로 변환하는 구조

- 인코더(Encoder)와 디코더(Decoder)로 구성

### 1. 인코더

- 입력 시퀀스를 받아 고정된 길이의 벡터(Context Vector)로 변환

- 문장의 정보를 압축적으로 표현



### 2. 디코더

- 인코더에서 생성한 문맥 벡터(Context Vector)를 기반으로 출력 시퀀스를 생성

- 이전 출력과 문맥 벡터를 사용하여 다음 출력을 예측

### 참고

- 인코더, 디코더에 다양한 모델이 올 수 있음 (RNN, Attention, Self-Attention 등)

- 구조적인 관점에서 자연언어처리 대부분은 Encoder only, Decoder only, Encoder-Decoder 구조라는 이 3가지 방식을 벗어나는 경우 거의 없음

- 자연언어를 이해하는 Encoder 부분과 자연언어 생성을 하는 Decoder 부분으로 나뉨

<br>

# RNN

- Recurrent Neural Networks

- 시퀀스 데이터를 다루기 위한 신경망 구조로, 이전 시간 스텝의 정보를 현재 스텝에 전달하여 시퀀스를 모델링함

<br>

- 수식 

$$
h_t = \sigma(W_h h_{t-1} + W_x x_t)
$$

$$
y_t = g(W_y h_t)
$$

- 수식 설명

    - **\( h_t \)** : 현재 은닉 상태 (메모리 역할)
    - **\( W_h, W_x, W_y \)** : 학습 가능한 가중치
    - **\( y_t \)** : 현재 시점의 출력
    - **\( \sigma, g \)** : 활성화 함수 (보통 tanh, softmax)

<br>

- RNN의 주요 문제점

    - Long-term dependency 문제: 긴 시퀀스를 다룰 때, 과거 정보가 후반부로 잘 전달되지 않음

    - Gradient Vanishing/Exploding: 역전파 시 기울기가 너무 작아지거나 커지는 문제

<br>

# LSTM

- Long Short-Term Memoery

- RNN의 단점을 개선한 구조로, 중요한 정보를 오랫동안 기억할 수 있도록 설계됨

- cell state와 gate라는 매커니즘을 도입

- cell state: 신경망에서 장기 기억을 저장하는 주요 요소

- 3가지 게이트(Foreget, Input, Output)가 있으며, 이들이 cell state를 조절함

- 수식

<br>

Forget Gate: 불필요한 정보를 제거

$$
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
$$

Input Gate: 새로운 정보를 추가

$$
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
$$

Candidate Cell State

$$
 \tilde{C}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)
$$

최종 Cell State 업데이트

$$
C_t = f_t C_{t-1} + i_t \tilde{C}_t
$$

Output Gate: 최종적인 출력값 결정

$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
$$

Hidden State 업데이트:

$$
h_t = o_t \tanh(C_t)
$$

-  수식 설명

    - **\( f_t \)** : Forget Gate의 활성화 값
    - **\( i_t \)** : Input Gate의 활성화 값
    - **\( C_t \)** : 현재 셀 상태 (Cell State)
    - **\( o_t \)** : Output Gate의 활성화 값
    - **\( h_t \)** : 현재 은닉 상태 (Hidden State)
    - **\( W_f, W_i, W_o, W_c \)** : 학습 가능한 가중치 행렬
    - **\( b_f, b_i, b_o, b_c \)** : 각 게이트의 편향값 (Bias)

<br>

# GRU

- Gated Recurrent Unit

- LSTM의 간소화 버전으로, 계산 효율성이 높은 구조를 가짐

- 2개의 gate(update, reset)와 하나의 상태

- 수식

<br>

Update Gate: 새로운 정보와 이전 정보를 얼마나 섞을지 결정

$$
z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)
$$

Reset Gate: 이전 정보를 얼마나 반영할지 결정

$$
r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)
$$

Candidate Hidden State

$$
 \tilde{h}_t = \tanh(W_h [r_t \cdot h_{t-1}, x_t] + b_h)
$$

최종 Hidden State 업데이트

$$
h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t
$$

<br>

- 수식 설명

    - **\( z_t \)** : Update Gate의 활성화 값
    - **\( r_t \)** : Reset Gate의 활성화 값
    - **\( \tilde{h}_t \)** : Candidate Hidden State
    - **\( h_t \)** : 현재 은닉 상태 (Hidden State)
    - **\( W_z, W_r, W_h \)** : 학습 가능한 가중치 행렬
    - **\( b_z, b_r, b_h \)** : 각 게이트의 편향값 (Bias)

<br>

# RNN, LSTM, GRU 비교

| 모델  | 게이트 수 | Long-term Dependency | 계산 효율성 |
|-------|----------|--------------|------------|
| **RNN**  | 없음  | 낮음 | 높음 |
| **LSTM** | 3개 (Forget, Input, Output) | 높음 | 낮음 |
| **GRU**  | 2개 (Update, Reset) | 중간 | 중간 |

