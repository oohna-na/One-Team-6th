# Contents
- [BERT](#bert)
    - [BERT 개요](#bert-개요)
    - [BERT 모델 구조](#bert-모델-구조)
    - [BERT 학습 방법](#bert-학습-방법)
    - [BERT Downstream Tasks](#bert-downstream-tasks)

<br>

# BERT

- 자연어 처리(NLP)에서 문맥을 더 잘 이해하도록 설계된 Transformer 기반 모델

<br>

## BERT 개요

- BERT (Bidirectional Encoder Representations from Transformers)

- Transformer의 Encoder만을 사용한 NLP 모델

- 양방향(Bidirectional)으로 문장을 이해하여 더 정확한 의미 파악 가능

- 기존 모델과의 차이점

    - 기존 NLP 모델(RNN, LSTM)은 단어를 한 방향(좌->우, 우->좌)으로만 처리

    - BERT는 양방향(Bidirectional)으로 문자을 처리하여 문맥을 더 잘 이해

- BERT의 주요 특징

    - 문장에서 랜덤으로 단어를 마스킹하고, 이를 예측하는 방식(Masked Language Model, MLM)으로 학습

    - 두 문장이 연속되는 문장인지 예측하는 Task(Next Sentence Prediction, NSP) 수행

    - 사전 학습(Pretraining) 후, 감성 분석, 질의 응답(QA), 개체명 인식(NER) 등의 Task에 Fine-tuning 가능

<br>

## BERT 모델 구조

-  BERT-Base vs BERT-Large

    |모델|파라미터 수|Encoder Layer 수|Hidden Unit 수|Attention Head 수|
    |------|---|---|---|---|
    |BERT-Base|1.1억|12|768|12|
    |BERT-Large|3.4억|24|1024|16|

    -> BERT-Large가 더 깊고 크지만 연산량이 많아 학습 속도가 느림

- 입력 구조

    |토큰|역할|
    |------|---|
    |[CLS]|문장의 시작을 나타내는 토큰 (문장 분류에 사용)|
    |토큰 임베딩|각 단어를 벡터로 변환한 값|
    |Segment Embedding|여러 문장이 들어올 경우, 서로 다른 문장 구분|
    |Position Embedding|문장 내 단어의 순서 정보 반영|

<br>

## BERT 학습 방법

1. Masked Language Model (MLM)

    - 문장에서 랜덤한 단어를 [MASK]로 가리고 이를 예측하도록 학습

    - ex.

        - 입력: "I love [MASK] and pizza."

        - 예측: ```"[MASK]" -> burgers```

    - BERT는 양방향(Bidirectional) 문맥을 활용하여 예측하므로 더 정확한 언어 모델 학습 가능

2. Next Sentence Prediction (NSP)

    - 두 문장이 연속된 문장인지 예측하는 Task

    - ex.

        - Sentence A: "I went to the store."

        - Sentence B: "I bought some milk." -> 연속된 문장

        - Sentence C: "The sky is blue." -> 연속되지 않음

    - 질의응답(QA), 문장 연결성 판단 등에 활용 가능

<br>

## BERT Downstream Tasks

- BERT는 사전 학습 후, 다양한 NLP 작업(Task)에 Fine-tuning하여 적용됨

1. 문장 분류 (Single Sentence Classification)

    - 감성 분석(Sentiment Analysis), 가짜 뉴스 판별
    
    - [CLS] 토큰을 화용하여 문장 전체의 의미를 분류

2. 두 문장 관계 예측 (Sentence Pair Classification)

    - 자연어 추론(NLI), 질문 응답(QA)

    - [CLS] 토큰을 사용하여 두 문장 간 관계(연관성, 모순)를 예측

3. 질문 응답 (Question Answering)

    - 문장에서 답변을 찾아내는 Task (ex. SQuAD 데이터셋)

    - 문장 내에서 Start와 End 토큰을 예측하여 답변 위치 찾기

4. 개체명 인식 (Named Entity Recognition, NER)

    - 문장에서 특정 개체(사람, 장소, 조직 등)를 찾아냄

    - 각 단어 벡터를 분류하여 개체명 예측측