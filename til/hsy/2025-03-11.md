# Contents
- [Attention](#Attention)
- [Transformer](#Transformer)


<br>

# Attention

## Attention의 핵심 개념

- Attention은 인코더(Encoder)와 디코더(Decoder) 사이에서 동작

- 각 단어의 hidden state에 중요도를 계산하고, 더 중요한 단어에 집중

- 이 점수를 기반으로 새로운 문맥 벡터(Context Vector)를 생성

- 즉, 문장의 모든 단어를 동일하게 처리하는 것이 아니라, 중요한 단어에 더 집중하도록 만듦

<br>

## Attention의 동작 방식

1. Query, Key, Value 개념
    
    - Query: 현재 예측하려는 단어의 정보 (디코더에서 현재 보고 있는 단어)

    - Key: 입력 문장(인코더)의 각 단어

    - Value: 입력 문장의 실제 내용(벡터)

    - ex. ```"나는 축구를 좋아한다."``` 에서 ```"나는"```이 ```"축구"```와 관련이 있는지 판단

        - 이때, ```"나는"```을 Query로 사용하여 ```"축구"```. ```"를'```, ```"좋아한다"``` 등의 단어들(Key)과 비교하여 관련성을 찾음

        - 관련성이 높은 단어는 Value에서 중요한 정보로 반영됨

        |개념|설명|예제|
        |------|---|---|
        |Query (질문)|비교 기준이 되는 단어|```"나는"```|
        |Key (비교할 정보)|문장의 모든 단어들|```"축구"```, ```"를"```, ```"좋아한다"```|
        |Value (결과 정보)|최종적으로 반영될 정보|```"축구를 좋아한다"```의 의미|


    - Query와 Key 사이의 유사도를 계산하여, Value에서 어떤 단어가 중요한지를 판단함

    - 이 점수를 Attention Score라고 하며, Softmax를 사용해 가중치를 계산함

2. Attention Score 계산

    1) Query와 Key의 유사도를 계산

    2) Softmax를 적용해 확률로 변환 (총합 1)

    3) 이 확률을 각 단어의 Value에 곱해서 최종 문맥(Context Vector) 생성

    -> 즉, 문맥을 더 잘 반영한 벡터를 만들고, 이를 이용해 다음 단어를 예측

<br>

## Attention의 종류

1. Bahdanau Attention (Additive Attention)

    - RNN 기반 번역 모델에서 처음 사용된 Attention 방식

    - Query와 Key를 MLP(신경망)로 변환하여 점수를 계산

2. Dot-Product Attention

    - Query와 Key의 내적(dot product)을 사용하여 점수를 계산
    
    - 연산이 빠르고 효율적 -> Transformer 모델에서 사용됨됨

<br>

# Transformer

## Transformer의 핵심 개념

1. Self-Attention

    - 문장에서 중요한 단어를 찾고, 그 단어들끼리의 관계를 파악하는 과정

    - ex. ```"나는 어제 축구를 했다."```라는 문장에서 ```"축구"```가 ```"했다"```와 연관이 높음

    - Transformer는 모든 단어가 서로 어떤 관계인지 점수를 매기고 계산함

2. Multi-Head Attention

    - Self-Attention을 여러 번 반복하여 다양한 의미를 학습

    - ex. ```"나는 어제 축구를 했다."```에서 ```"나는"```이 ```"했다"```와도 연결되고, ```"축구"```와도 연결됨

    - 한 번만 계산하는 것이 아니라 여러 개의 다른 관점에서 Self-Attention을 계산

3. Positional Encoding

    - RNN과 달리, Transformer는 순서 정보를 모르기 때문에 단어의 위치 정보를 추가로 넣어줌

    - ex. ```"나는"```이 문장의 맨 앞에 오는 것도 ```"나는"```이 뒤에 오는 것은 의미가 다름

    - Positional Encoding을 통해 단어의 순서 정보도 반영함

<br>

## Transformer의 구조

1. Encoder (인코더)

    - 입력 문장을 처리하고, 문맥을 이해하는 역할

    - 여러 개의 Self-Attention 레이어와 Feed-Forward Network(FFN)으로 구성

2. Decoder (디코더)

    - 인코더에서 받은 정보를 바탕으로 새로운 문장을 생성하는 역할

    - Masked Self-Attnetion을 사용해 아직 출력되지 않은 단어는 참고하지 않도록 함

    - 번역이나 문장 생성에서 출력 문장을 한 글자씩 만들어냄